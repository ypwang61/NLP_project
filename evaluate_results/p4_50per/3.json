{
  "ori_ocr_text": "Masked Self Attention",
  "ocr": "Meccket Seuenstntion",
  "result_list": [
    [
      "Masked Self-Attention",
      " closely matches the description in text and follows standard Transformer architecture"
    ],
    [
      "Cross-Attention",
      " as it's a component in the decoder following the self-attention layer"
    ],
    [
      "Add & Norm",
      " because each attention operation is followed by a residual connection and layer normalization"
    ]
  ],
  "edit_dis": {
    "ocr": 10,
    "top_1_guess": 1,
    "top_3_guess": 1,
    "top_1_reason": 74,
    "top_3_reason": 54
  },
  "rouge_score@rouge1": {
    "ocr": 0.0,
    "top_1_guess": 1.0,
    "top_3_guess": 1.0,
    "top_1_reason": 0.0,
    "top_3_reason": 0.25
  },
  "rouge_score@rouge2": {
    "ocr": 0.0,
    "top_1_guess": 1.0,
    "top_3_guess": 1.0,
    "top_1_reason": 0.0,
    "top_3_reason": 0.14285714285714285
  },
  "rouge_score@rougeL": {
    "ocr": 0.0,
    "top_1_guess": 1.0,
    "top_3_guess": 1.0,
    "top_1_reason": 0.0,
    "top_3_reason": 0.25
  },
  "rouge_score@rougeLsum": {
    "ocr": 0.0,
    "top_1_guess": 1.0,
    "top_3_guess": 1.0,
    "top_1_reason": 0.0,
    "top_3_reason": 0.25
  },
  "Bert_sim@precision": {
    "ocr": 0.3600555658340454,
    "top_1_guess": 0.7130242586135864,
    "top_3_guess": 0.7130242586135864,
    "top_1_reason": 0.3450791537761688,
    "top_3_reason": 0.37306931614875793
  },
  "Bert_sim@recall": {
    "ocr": 0.4613545536994934,
    "top_1_guess": 0.8206989169120789,
    "top_3_guess": 0.8206989169120789,
    "top_1_reason": 0.41741275787353516,
    "top_3_reason": 0.5702113509178162
  },
  "Bert_sim@f1": {
    "ocr": 0.40445879101753235,
    "top_1_guess": 0.7630820274353027,
    "top_3_guess": 0.7630820274353027,
    "top_1_reason": 0.37781497836112976,
    "top_3_reason": 0.4510393440723419
  }
}