{
  "ori_ocr_text": "Add",
  "ocr": "Add.",
  "result_list": [
    [
      "Add & Norm",
      " appears consistently with similar blocks in the architecture"
    ],
    [
      "Residual Connection",
      " commonly follows attention mechanisms in Transformer models"
    ],
    [
      "Layer Normalization",
      " typically paired with Add & Norm in Transformer architectures"
    ]
  ],
  "edit_dis": {
    "ocr": 1,
    "top_1_guess": 7,
    "top_3_guess": 7,
    "top_1_reason": 61,
    "top_3_reason": 59
  },
  "Bert_sim": {
    "ocr": 0.6485083699226379,
    "top_1_guess": 0.5827000141143799,
    "top_3_guess": 0.5827000141143799,
    "top_1_reason": 0.33713075518608093,
    "top_3_reason": 0.3721080720424652
  },
  "rouge_score@rouge1": {
    "ocr": 1.0,
    "top_1_guess": 0.6666666666666666,
    "top_3_guess": 0.6666666666666666,
    "top_1_reason": 0.0,
    "top_3_reason": 0.2222222222222222
  },
  "rouge_score@rouge2": {
    "ocr": 0.0,
    "top_1_guess": 0.0,
    "top_3_guess": 0.0,
    "top_1_reason": 0.0,
    "top_3_reason": 0.0
  },
  "rouge_score@rougeL": {
    "ocr": 1.0,
    "top_1_guess": 0.6666666666666666,
    "top_3_guess": 0.6666666666666666,
    "top_1_reason": 0.0,
    "top_3_reason": 0.2222222222222222
  },
  "rouge_score@rougeLsum": {
    "ocr": 1.0,
    "top_1_guess": 0.6666666666666666,
    "top_3_guess": 0.6666666666666666,
    "top_1_reason": 0.0,
    "top_3_reason": 0.2222222222222222
  }
}