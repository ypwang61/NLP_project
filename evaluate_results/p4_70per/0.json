{
  "ori_ocr_text": "Add",
  "ocr": "Add",
  "result_list": [
    [
      "Layer Normalization",
      " it typically follows operations like 'Add' in a Transformer architecture"
    ],
    [
      "Residual Connection",
      " as it is often accompanied by layer normalization"
    ],
    [
      "Add & Norm",
      " because this is a common label for the combination of residual add and normalization"
    ]
  ],
  "edit_dis": {
    "ocr": 0,
    "top_1_guess": 19,
    "top_3_guess": 7,
    "top_1_reason": 70,
    "top_3_reason": 49
  },
  "Bert_sim": {
    "ocr": 0.9999999403953552,
    "top_1_guess": 0.3258386552333832,
    "top_3_guess": 0.5827000141143799,
    "top_1_reason": 0.3771968185901642,
    "top_3_reason": 0.3771968185901642
  },
  "rouge_score@rouge1": {
    "ocr": 1.0,
    "top_1_guess": 0.0,
    "top_3_guess": 0.6666666666666666,
    "top_1_reason": 0.18181818181818182,
    "top_3_reason": 0.18181818181818182
  },
  "rouge_score@rouge2": {
    "ocr": 0.0,
    "top_1_guess": 0.0,
    "top_3_guess": 0.0,
    "top_1_reason": 0.0,
    "top_3_reason": 0.0
  },
  "rouge_score@rougeL": {
    "ocr": 1.0,
    "top_1_guess": 0.0,
    "top_3_guess": 0.6666666666666666,
    "top_1_reason": 0.18181818181818182,
    "top_3_reason": 0.18181818181818182
  },
  "rouge_score@rougeLsum": {
    "ocr": 1.0,
    "top_1_guess": 0.0,
    "top_3_guess": 0.6666666666666666,
    "top_1_reason": 0.18181818181818182,
    "top_3_reason": 0.18181818181818182
  }
}